{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "close-strip",
   "metadata": {},
   "source": [
    "# Задание 4\n",
    "\n",
    "Условие см. <a href=\"https://docs.google.com/document/d/1Y2DCQ0WxmLFtyu33ddhCQpAxhGmo8tjYhbuQ-sdlhuQ/edit#\">здесь</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "synthetic-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_TRAIN = \"/datasets/amazon/all_reviews_5_core_train.json\" # - большой тренировочный датасет примерно на 20 миллионов записей.\n",
    "SMALL_TRAIN = \"/datasets/amazon/all_reviews_5_core_train_small.json\" # - маленький тренировочный датасет на 1 миллион записей.\n",
    "TEST = \"/datasets/amazon/all_reviews_5_core_test_features.json\" # - тестовый датасет на примерно 83 миллиона записей.\n",
    "TRAIN = SMALL_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "progressive-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME = \"/usr/hdp/current/spark2-client\"\n",
    "PYSPARK_PYTHON = \"/opt/conda/envs/dsenv/bin/python\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]= PYSPARK_PYTHON\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "PYSPARK_HOME = os.path.join(SPARK_HOME, \"python/lib\")\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"py4j-0.10.7-src.zip\"))\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"pyspark.zip\"))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "professional-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Estimator, Transformer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foster-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"overall\", IntegerType()),\n",
    "    StructField(\"vote\", StringType()),\n",
    "    StructField(\"verified\", StringType ()),\n",
    "    StructField(\"reviewTime\", StringType()),\n",
    "    StructField(\"reviewerID\", StringType()),\n",
    "    StructField(\"asin\", StringType()),\n",
    "    StructField(\"reviewerName\", StringType()),\n",
    "    StructField(\"reviewText\", StringType()),\n",
    "    StructField(\"summary\", StringType()),\n",
    "    StructField(\"unixReviewTime\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-guatemala",
   "metadata": {},
   "source": [
    "Некоторые столбцы вряд ли понадобятся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "electronic-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.json(TRAIN)\n",
    "dataset = dataset.drop(\"image\", \"reviewerName\", \"unixReviewTime\").cache()\n",
    "# id пока не дропаем - ещё пригодится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "pharmaceutical-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " asin       | B00005MDZ8           \n",
      " id         | 6500                 \n",
      " overall    | 5.0                  \n",
      " reviewText | quick shipping, g... \n",
      " reviewTime | 10 23, 2014          \n",
      " reviewerID | AEZ4DZCUL021H        \n",
      " summary    | great product        \n",
      " verified   | true                 \n",
      " vote       | null                 \n",
      "-RECORD 1--------------------------\n",
      " asin       | B000DZE0XK           \n",
      " id         | 42580                \n",
      " overall    | 5.0                  \n",
      " reviewText | Most delicious Ever! \n",
      " reviewTime | 02 13, 2016          \n",
      " reviewerID | A3UPMJ5WQFHGLN       \n",
      " summary    | Five Stars           \n",
      " verified   | true                 \n",
      " vote       | null                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "serial-playlist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(asin,StringType,true),StructField(overall,DoubleType,true),StructField(reviewText,StringType,true),StructField(reviewTime,StringType,true),StructField(reviewerID,StringType,true),StructField(summary,StringType,true),StructField(verified,BooleanType,true),StructField(vote,StringType,true)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-disease",
   "metadata": {},
   "source": [
    "Здесь всяческие преобразования над датасетом. Как минимум - vote и verified конвертируем в Int,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "funded-store",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-b67116537da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"verified\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vote\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dsenv/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "verified = f.when(dataset.verified, 1).otherwise(0)\n",
    "vote = f.when(dataset.vote.isNull(), 0).otherwise(dataset.vote.astype(IntegerType()))\n",
    "\n",
    "dataset = dataset.withColumn(\"verified\", verified)\n",
    "dataset = dataset.withColumn(\"vote\", vote)\n",
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-vitamin",
   "metadata": {},
   "source": [
    "Продолжим с цинизмом. Удалим всё лишнее, кроме чисел и summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(\"asin\", \"reviewTime\", \"reviewerID\", \"summary\").cache()\n",
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-vegetarian",
   "metadata": {},
   "source": [
    "## Обработаем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "tokenizer = RegexTokenizer(minTokenLength=2, pattern='\\\\W', inputCol=\"reviewText\", outputCol=\"words\")\n",
    "dataset2 = tokenizer.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.select(\"words\").show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(vocabSize=5 * 10e3, inputCol=tokenizer.getOutputCol(), outputCol=\"cv\")\n",
    "cv_model = cv.fit(dataset2)\n",
    "dataset2 = cv_model.transform(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.select(\"cv\").show(2, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-guard",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "primary-thursday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|overall|   count|\n",
      "+-------+--------+\n",
      "|    1.0| 1781920|\n",
      "|    4.0| 3215822|\n",
      "|    3.0| 1604121|\n",
      "|    2.0| 1054679|\n",
      "|    5.0|12996336|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2.groupBy(\"overall\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "potential-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.8\n",
    "train = dataset2.sampleBy(\"overall\", fractions={1.0: test_frac, 2.0: test_frac,\n",
    "                                                3.0: test_frac, 4.0: test_frac, 5.0: test_frac}, seed=5757)\n",
    "test = dataset2.join(train, on=\"id\", how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "accepting-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|overall|   count|\n",
      "+-------+--------+\n",
      "|    1.0| 1425736|\n",
      "|    4.0| 2572497|\n",
      "|    3.0| 1282758|\n",
      "|    2.0|  843440|\n",
      "|    5.0|10397881|\n",
      "+-------+--------+\n",
      "\n",
      "+-------+-------+\n",
      "|overall|  count|\n",
      "+-------+-------+\n",
      "|    1.0| 356184|\n",
      "|    4.0| 643325|\n",
      "|    3.0| 321363|\n",
      "|    2.0| 211239|\n",
      "|    5.0|2598455|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy(\"overall\").count().show(), test.groupBy(\"overall\").count().show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "convertible-rating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop(\"id\").cache()\n",
    "test = test.drop(\"id\").coalesce(train.rdd.getNumPartitions()).cache()\n",
    "train.rdd.getNumPartitions(), test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "convinced-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "polyphonic-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=cv.getOutputCol(), labelCol=\"overall\", maxIter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "departmental-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90.2 ms, sys: 38.5 ms, total: 129 ms\n",
      "Wall time: 9min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "coordinated-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "earned-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " overall    | 1.0                \n",
      " prediction | 2.909890589945199  \n",
      "-RECORD 1------------------------\n",
      " overall    | 5.0                \n",
      " prediction | 4.310055731339988  \n",
      "-RECORD 2------------------------\n",
      " overall    | 5.0                \n",
      " prediction | 4.1937918153804254 \n",
      "-RECORD 3------------------------\n",
      " overall    | 5.0                \n",
      " prediction | 4.2805712601666395 \n",
      "-RECORD 4------------------------\n",
      " overall    | 4.0                \n",
      " prediction | 4.520410780292606  \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"overall\", \"prediction\").show(5, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "organic-carnival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9731297882932891"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-clinic",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fifty-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    cv,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "inner-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 158 ms, sys: 48.9 ms, total: 207 ms\n",
      "Wall time: 9min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9730072412432708"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TRAIN = BIG_TRAIN\n",
    "dataset = spark.read.json(TRAIN)\n",
    "dataset = dataset.drop(\"image\", \"reviewerName\", \"unixReviewTime\").cache()\n",
    "\n",
    "verified = f.when(dataset.verified, 1).otherwise(0)\n",
    "vote = f.when(dataset.vote.isNull(), 0).otherwise(dataset.vote.astype(IntegerType()))\n",
    "\n",
    "dataset = dataset.withColumn(\"verified\", verified)\n",
    "dataset = dataset.withColumn(\"vote\", vote)\n",
    "dataset = dataset.drop(\"asin\", \"reviewTime\", \"reviewerID\", \"summary\").cache()\n",
    "\n",
    "test_frac = 0.8\n",
    "train = dataset.sampleBy(\"overall\", fractions={1.0: test_frac, 2.0: test_frac,\n",
    "                                                3.0: test_frac, 4.0: test_frac, 5.0: test_frac}, seed=5757)\n",
    "test = dataset.join(train, on=\"id\", how=\"leftanti\")\n",
    "\n",
    "train = train.drop(\"id\").cache()\n",
    "test = test.drop(\"id\").coalesce(train.rdd.getNumPartitions()).cache()\n",
    "\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "radical-engineering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/datasets/amazon/all_reviews_5_core_train.json'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "manufactured-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save('pipeline_model.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
