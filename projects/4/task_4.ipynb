{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legislative-sullivan",
   "metadata": {},
   "source": [
    "# Задание 4\n",
    "\n",
    "Условие см. <a href=\"https://docs.google.com/document/d/1Y2DCQ0WxmLFtyu33ddhCQpAxhGmo8tjYhbuQ-sdlhuQ/edit#\">здесь</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brief-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_TRAIN = \"/datasets/amazon/all_reviews_5_core_train.json\" # - большой тренировочный датасет примерно на 20 миллионов записей.\n",
    "SMALL_TRAIN = \"/datasets/amazon/all_reviews_5_core_train_small.json\" # - маленький тренировочный датасет на 1 миллион записей.\n",
    "TEST = \"/datasets/amazon/all_reviews_5_core_test_features.json\" # - тестовый датасет на примерно 83 миллиона записей.\n",
    "TRAIN = SMALL_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sitting-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME = \"/usr/hdp/current/spark2-client\"\n",
    "PYSPARK_PYTHON = \"/opt/conda/envs/dsenv/bin/python\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]= PYSPARK_PYTHON\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "PYSPARK_HOME = os.path.join(SPARK_HOME, \"python/lib\")\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"py4j-0.10.7-src.zip\"))\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"pyspark.zip\"))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "referenced-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Estimator, Transformer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wooden-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"overall\", IntegerType()),\n",
    "    StructField(\"vote\", StringType()),\n",
    "    StructField(\"verified\", StringType ()),\n",
    "    StructField(\"reviewTime\", StringType()),\n",
    "    StructField(\"reviewerID\", StringType()),\n",
    "    StructField(\"asin\", StringType()),\n",
    "    StructField(\"reviewerName\", StringType()),\n",
    "    StructField(\"reviewText\", StringType()),\n",
    "    StructField(\"summary\", StringType()),\n",
    "    StructField(\"unixReviewTime\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-artwork",
   "metadata": {},
   "source": [
    "Некоторые столбцы вряд ли понадобятся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "loaded-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.json(TRAIN)\n",
    "dataset = dataset.drop(\"image\", \"reviewerName\", \"unixReviewTime\").cache()\n",
    "# id пока не дропаем - ещё пригодится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "capable-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " asin       | B00005MDZ8           \n",
      " id         | 6500                 \n",
      " overall    | 5.0                  \n",
      " reviewText | quick shipping, g... \n",
      " reviewTime | 10 23, 2014          \n",
      " reviewerID | AEZ4DZCUL021H        \n",
      " summary    | great product        \n",
      " verified   | true                 \n",
      " vote       | null                 \n",
      "-RECORD 1--------------------------\n",
      " asin       | B000DZE0XK           \n",
      " id         | 42580                \n",
      " overall    | 5.0                  \n",
      " reviewText | Most delicious Ever! \n",
      " reviewTime | 02 13, 2016          \n",
      " reviewerID | A3UPMJ5WQFHGLN       \n",
      " summary    | Five Stars           \n",
      " verified   | true                 \n",
      " vote       | null                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "choice-greece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(asin,StringType,true),StructField(overall,DoubleType,true),StructField(reviewText,StringType,true),StructField(reviewTime,StringType,true),StructField(reviewerID,StringType,true),StructField(summary,StringType,true),StructField(verified,BooleanType,true),StructField(vote,StringType,true)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-collective",
   "metadata": {},
   "source": [
    "Здесь всяческие преобразования над датасетом. Как минимум - vote и verified конвертируем в Int,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "material-folder",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-b67116537da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"verified\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vote\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/dsenv/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "verified = f.when(dataset.verified, 1).otherwise(0)\n",
    "vote = f.when(dataset.vote.isNull(), 0).otherwise(dataset.vote.astype(IntegerType()))\n",
    "\n",
    "dataset = dataset.withColumn(\"verified\", verified)\n",
    "dataset = dataset.withColumn(\"vote\", vote)\n",
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-focus",
   "metadata": {},
   "source": [
    "Продолжим с цинизмом. Удалим всё лишнее, кроме чисел и summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(\"asin\", \"reviewTime\", \"reviewerID\", \"summary\").cache()\n",
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-polymer",
   "metadata": {},
   "source": [
    "## Обработаем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "tokenizer = RegexTokenizer(minTokenLength=2, pattern='\\\\W', inputCol=\"reviewText\", outputCol=\"words\")\n",
    "dataset2 = tokenizer.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.select(\"words\").show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(vocabSize=5 * 10e3, inputCol=tokenizer.getOutputCol(), outputCol=\"cv\")\n",
    "cv_model = cv.fit(dataset2)\n",
    "dataset2 = cv_model.transform(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.select(\"cv\").show(2, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-arlington",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "configured-corner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|overall|   count|\n",
      "+-------+--------+\n",
      "|    1.0| 1781920|\n",
      "|    4.0| 3215822|\n",
      "|    3.0| 1604121|\n",
      "|    2.0| 1054679|\n",
      "|    5.0|12996336|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2.groupBy(\"overall\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "closing-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.8\n",
    "train = dataset2.sampleBy(\"overall\", fractions={1.0: test_frac, 2.0: test_frac,\n",
    "                                                3.0: test_frac, 4.0: test_frac, 5.0: test_frac}, seed=5757)\n",
    "test = dataset2.join(train, on=\"id\", how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "manual-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|overall|   count|\n",
      "+-------+--------+\n",
      "|    1.0| 1425736|\n",
      "|    4.0| 2572497|\n",
      "|    3.0| 1282758|\n",
      "|    2.0|  843440|\n",
      "|    5.0|10397881|\n",
      "+-------+--------+\n",
      "\n",
      "+-------+-------+\n",
      "|overall|  count|\n",
      "+-------+-------+\n",
      "|    1.0| 356184|\n",
      "|    4.0| 643325|\n",
      "|    3.0| 321363|\n",
      "|    2.0| 211239|\n",
      "|    5.0|2598455|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy(\"overall\").count().show(), test.groupBy(\"overall\").count().show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "unexpected-farmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop(\"id\").cache()\n",
    "test = test.drop(\"id\").coalesce(train.rdd.getNumPartitions()).cache()\n",
    "train.rdd.getNumPartitions(), test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acknowledged-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "about-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=cv.getOutputCol(), labelCol=\"overall\", maxIter=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-portuguese",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(count_vectorizer.vocabSize, [100, 500])\\\n",
    "                              .addGrid(lr.regParam, [0.01, 0.05])\\\n",
    "                              .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator, numFolds=3, parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-smith",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "warming-petroleum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90.2 ms, sys: 38.5 ms, total: 129 ms\n",
      "Wall time: 9min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "indonesian-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "built-venezuela",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " overall    | 1.0                \n",
      " prediction | 2.909890589945199  \n",
      "-RECORD 1------------------------\n",
      " overall    | 5.0                \n",
      " prediction | 4.310055731339988  \n",
      "-RECORD 2------------------------\n",
      " overall    | 5.0                \n",
      " prediction | 4.1937918153804254 \n",
      "-RECORD 3------------------------\n",
      " overall    | 5.0                \n",
      " prediction | 4.2805712601666395 \n",
      "-RECORD 4------------------------\n",
      " overall    | 4.0                \n",
      " prediction | 4.520410780292606  \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"overall\", \"prediction\").show(5, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "saving-alpha",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9731297882932891"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-crisis",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "accompanied-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    cv,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "suburban-momentum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 158 ms, sys: 48.9 ms, total: 207 ms\n",
      "Wall time: 9min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9730072412432708"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TRAIN = BIG_TRAIN\n",
    "dataset = spark.read.json(TRAIN)\n",
    "dataset = dataset.drop(\"image\", \"reviewerName\", \"unixReviewTime\").cache()\n",
    "\n",
    "verified = f.when(dataset.verified, 1).otherwise(0)\n",
    "vote = f.when(dataset.vote.isNull(), 0).otherwise(dataset.vote.astype(IntegerType()))\n",
    "\n",
    "dataset = dataset.withColumn(\"verified\", verified)\n",
    "dataset = dataset.withColumn(\"vote\", vote)\n",
    "dataset = dataset.drop(\"asin\", \"reviewTime\", \"reviewerID\", \"summary\").cache()\n",
    "\n",
    "test_frac = 0.8\n",
    "train = dataset.sampleBy(\"overall\", fractions={1.0: test_frac, 2.0: test_frac,\n",
    "                                                3.0: test_frac, 4.0: test_frac, 5.0: test_frac}, seed=5757)\n",
    "test = dataset.join(train, on=\"id\", how=\"leftanti\")\n",
    "\n",
    "train = train.drop(\"id\").cache()\n",
    "test = test.drop(\"id\").coalesce(train.rdd.getNumPartitions()).cache()\n",
    "\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "premier-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/datasets/amazon/all_reviews_5_core_train.json'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "driving-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save('pipeline_model.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-cleanup",
   "metadata": {},
   "source": [
    "# Применение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attached-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"pipeline_model_task_4.mdl\"\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "model = PipelineModel.load(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pharmaceutical-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.json(TEST)\n",
    "dataset = dataset.drop(\"asin\", \"reviewTime\", \"reviewerID\", \"summary\", \"image\", \"reviewerName\", \"unixReviewTime\").cache()\n",
    "\n",
    "verified = f.when(dataset.verified, 1).otherwise(0)\n",
    "vote = f.when(dataset.vote.isNull(), 0).otherwise(dataset.vote.astype(IntegerType()))\n",
    "\n",
    "dataset = dataset.withColumn(\"verified\", verified)\n",
    "dataset = dataset.withColumn(\"vote\", vote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "complimentary-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------+----+\n",
      "|           id|          reviewText|verified|vote|\n",
      "+-------------+--------------------+--------+----+\n",
      "|1692217575992|Very nice product...|       1|  14|\n",
      "|1692217575994|I bought one for ...|       1|   0|\n",
      "+-------------+--------------------+--------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "critical-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.\n",
    "dataset = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "virgin-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------+----+--------------------+--------------------+-----------------+\n",
      "|           id|          reviewText|verified|vote|               words|                  cv|       prediction|\n",
      "+-------------+--------------------+--------+----+--------------------+--------------------+-----------------+\n",
      "|1692217575992|Very nice product...|       1|  14|[very, nice, prod...|(50000,[0,1,2,3,6...|4.244987020518314|\n",
      "|1692217575994|I bought one for ...|       1|   0|[bought, one, for...|(50000,[1,5,8,27,...|4.486542680968824|\n",
      "+-------------+--------------------+--------+----+--------------------+--------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "objective-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DATA_PATH = 'predictions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "documentary-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset.write.json(OUTPUT_DATA_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "central-mission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 items\n",
      "drwx------   - vova-cmc vova-cmc          0 2021-03-31 00:00 .Trash\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-04-02 14:28 .hiveJars\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-04-21 14:10 .sparkStaging\n",
      "drwx------   - vova-cmc vova-cmc          0 2021-03-21 13:17 .staging\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-04-02 15:08 hive\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-03-29 07:52 hw3_output\n",
      "drwx------   - vova-cmc vova-cmc          0 2021-03-16 13:35 hwzero\n",
      "-rw-r--r--   3 vova-cmc vova-cmc          0 2021-03-16 14:30 hwzero.txt\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-03-21 12:28 input\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-04-18 18:50 pipeline_model.mdl\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-04-21 13:55 pipeline_model_task_4.mdl\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-04-18 19:19 pipeline_model_task_4_small.mdl\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-04-20 16:08 predictions.json\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-03-21 13:17 res1\n",
      "drwxr-xr-x   - vova-cmc vova-cmc          0 2021-03-21 13:16 res2\n",
      "-rw-r--r--   3 vova-cmc vova-cmc         47 2021-03-30 15:44 vova-cmc_hw3_output\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "working-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
