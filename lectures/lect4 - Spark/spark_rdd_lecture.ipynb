{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мотивация создания Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассмотрим два примера приложений:\n",
    "- Обучить модель на больших данных (читай итеративный алгоритм над фиксированным датасетом)\n",
    "- Провести ad-hoc анализ данных из двух таблиц (читай несколько интерактивных запросов с джойнами)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные недостатки классического MapReduce\n",
    "- Быстроумирающие контейнеры\n",
    "- Постоянное чтение/запись во внешнее хранилище\n",
    "- Сложный API\n",
    "- Ограниченное число источников/приемников данных\n",
    "- MapReduce - это только вычислительный фреймворк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark - это *быстрая* распределенная вычислительная платформа *общего назначения*\n",
    "1. **Быстрая** - это в памяти и с ленивыми вычислениями\n",
    "2. **Общего назначения** - значит на ней можно реализовать любые вычисления (батчевые, интерактивные, итеративные, в режиме реального времени)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/spark_stack.png\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Множество источников данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/spark_data_sources.jpg\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/cluster-overview.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME = \"/usr/hdp/current/spark2-client\"\n",
    "PYSPARK_PYTHON = \"/opt/conda/envs/dsenv/bin/python\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]= PYSPARK_PYTHON\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "PYSPARK_HOME = os.path.join(SPARK_HOME, \"python/lib\")\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"py4j-0.10.7-src.zip\"))\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"pyspark.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI port: 10269\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark_ui_port = random.choice(range(10000, 11000))\n",
    "print(f\"Spark UI port: {spark_ui_port}\")\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.ui.port\", spark_ui_port)\n",
    "\n",
    "sc = SparkContext(appName=\"Lecture 01\", conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext (sc) - это основной управляющий объект."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://edge1.ru-central1.internal:10349\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lecture 01</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=Lecture 01>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для получения всех установленных опций конфигурации можно использовать `sc.getConf()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.app.id', 'application_1613052795737_4264'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.history.fs.cleaner.interval', '7d'),\n",
       " ('spark.shuffle.service.port', '7447'),\n",
       " ('spark.shuffle.io.serverThreads', '128'),\n",
       " ('spark.sql.streaming.streamingQueryListeners', ''),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.shuffle.file.buffer', '1m'),\n",
       " ('spark.sql.hive.convertMetastoreOrc', 'true'),\n",
       " ('spark.yarn.dist.files', ''),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '26214400'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.driver.port', '33169'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.orc.impl', 'native'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.yarn.queue', 'queue2'),\n",
       " ('spark.history.fs.cleaner.maxAge', '90d'),\n",
       " ('spark.driver.host', 'edge1.ru-central1.internal'),\n",
       " ('spark.extraListeners', ''),\n",
       " ('spark.sql.warehouse.dir', '/apps/spark/warehouse'),\n",
       " ('spark.history.store.path', '/var/lib/spark2/shs_db'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://name1.ru-central1.internal:8088/proxy/application_1613052795737_4264'),\n",
       " ('spark.yarn.historyServer.address', 'name1.ru-central1.internal:18081'),\n",
       " ('spark.app.name', 'Lecture 01'),\n",
       " ('spark.sql.statistics.fallBackToHdfs', 'true'),\n",
       " ('spark.dynamicAllocation.cachedExecutorIdleTimeout', '600s'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '12'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.dynamicAllocation.executorIdleTimeout', '60s'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.extraClassPath', ''),\n",
       " ('spark.sql.hive.metastore.jars',\n",
       "  '/usr/hdp/current/spark2-client/standalone-metastore/*'),\n",
       " ('spark.driver.appUIAddress', 'http://edge1.ru-central1.internal:10349'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.ui.port', '10349'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1613052795737_4264'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.sql.queryExecutionListeners', ''),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.io.compression.lz4.blockSize', '128kb'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'name1.ru-central1.internal'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.sql.orc.filterPushdown', 'true'),\n",
       " ('spark.shuffle.io.backLog', '8192'),\n",
       " ('spark.unsafe.sorter.spill.reader.buffer.size', '1m'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.shuffle.unsafe.file.output.buffer', '5m'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.executor.extraJavaOptions', '-XX:+UseNUMA'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.hive.metastore.version', '3.0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Существует два способа создать RDD\n",
    "- распределить коллекцию объектов с драйвера\n",
    "- загрузить внешний датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Распределить коллекцию с драйвера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocabulary = (\"Apache\", \"Spark\", \"Hadoop\")\n",
    "numbers = np.random.randint(10, size=10000)\n",
    "words = np.random.choice(vocabulary, size=10000)\n",
    "collection = zip(numbers, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Apache'),\n",
       " (8, 'Apache'),\n",
       " (7, 'Spark'),\n",
       " (2, 'Spark'),\n",
       " (8, 'Hadoop'),\n",
       " (7, 'Apache'),\n",
       " (2, 'Hadoop'),\n",
       " (8, 'Hadoop'),\n",
       " (9, 'Spark'),\n",
       " (5, 'Hadoop')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузить внешний датасет (датасет загружается из HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 hdfs hdfs      19462 2021-03-11 13:43 /datasets/spark/ips.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /datasets/spark/ips.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.textFile(\"/datasets/spark/ips.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['192.168.0.1\\tCHINA',\n",
       " '192.168.0.2\\tCHINA',\n",
       " '192.168.0.3\\tCHINA',\n",
       " '192.168.0.4\\tCHINA',\n",
       " '192.168.0.5\\tCHINA',\n",
       " '192.168.0.6\\tCHINA',\n",
       " '192.168.0.7\\tCHINA',\n",
       " '192.168.0.8\\tCHINA',\n",
       " '192.168.0.9\\tCHINA',\n",
       " '192.168.0.10\\tCHINA']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD API состоит из операции двух типов:\n",
    "- action\n",
    "- transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформация преобразовывает RDD в другой RDD и не приводит к вычислениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action заставляет Spark произвести вычисления и вернуть результат либо на драйвер, либо во внешнее хранилище"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформации можно применять одну за другой, никаких вычислений не будет сделано, пока не будет вызван action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[14] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[13] at parallelize at PythonRDD.scala:195 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[17] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.filter(lambda x: x % 2)\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[17] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[13] at parallelize at PythonRDD.scala:195 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd2.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[18] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.map(lambda x: x * 2)\n",
    "rdd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[18] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[13] at parallelize at PythonRDD.scala:195 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd3.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 6,\n",
       " 10,\n",
       " 14,\n",
       " 18,\n",
       " 22,\n",
       " 26,\n",
       " 30,\n",
       " 34,\n",
       " 38,\n",
       " 42,\n",
       " 46,\n",
       " 50,\n",
       " 54,\n",
       " 58,\n",
       " 62,\n",
       " 66,\n",
       " 70,\n",
       " 74,\n",
       " 78,\n",
       " 82,\n",
       " 86,\n",
       " 90,\n",
       " 94,\n",
       " 98,\n",
       " 102,\n",
       " 106,\n",
       " 110,\n",
       " 114,\n",
       " 118,\n",
       " 122,\n",
       " 126,\n",
       " 130,\n",
       " 134,\n",
       " 138,\n",
       " 142,\n",
       " 146,\n",
       " 150,\n",
       " 154,\n",
       " 158,\n",
       " 162,\n",
       " 166,\n",
       " 170,\n",
       " 174,\n",
       " 178,\n",
       " 182,\n",
       " 186,\n",
       " 190,\n",
       " 194,\n",
       " 198]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как получить данные на драйвер и не выстрелить себе в ногу?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `take()` пытается минимизировать число обращений к партициям, поэтому может возвращать смещенные результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будьте аккуратны с `collect()`, потому что он загружает все данные из RDD на драйвер. Это может легко привести к `Out of Memory exception`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Если нужно получить небольшое число записей на драйвер и, при этом, сохранить распределение, то лучше сделать выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 75, 57, 18, 19, 96, 11, 55, 39, 40, 79, 26, 15, 13, 47, 7, 66, 86, 51, 12]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(withReplacement=False, num=20, seed=5757)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Познакомимся с данными. Будем работать с двумя таблицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/data_table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/data_table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры трансформаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/datasets/spark/ips.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['192.168.0.1\\tCHINA',\n",
       " '192.168.0.2\\tCHINA',\n",
       " '192.168.0.3\\tCHINA',\n",
       " '192.168.0.4\\tCHINA',\n",
       " '192.168.0.5\\tCHINA']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips = rdd.map(lambda x: x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.1', 'CHINA'],\n",
       " ['192.168.0.2', 'CHINA'],\n",
       " ['192.168.0.3', 'CHINA'],\n",
       " ['192.168.0.4', 'CHINA'],\n",
       " ['192.168.0.5', 'CHINA']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_filtered = ips.filter(lambda x: x[1] != \"CHINA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.201', 'RUSSIA'],\n",
       " ['192.168.0.202', 'RUSSIA'],\n",
       " ['192.168.0.203', 'RUSSIA'],\n",
       " ['192.168.0.204', 'RUSSIA'],\n",
       " ['192.168.0.205', 'RUSSIA']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_filtered.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_logs = sc.textFile(\"/datasets/spark/log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['192.168.0.10\\tERROR\\tWhen production fails in dipsair, whom you gonna call?',\n",
       " '192.168.0.39\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.35\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.19\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.23\\tERROR\\tWhen production fails in dipsair, whom you gonna call?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_logs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = raw_logs.map(lambda x: x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.10',\n",
       "  'ERROR',\n",
       "  'When production fails in dipsair, whom you gonna call?'],\n",
       " ['192.168.0.39', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.35', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.19', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.23',\n",
       "  'ERROR',\n",
       "  'When production fails in dipsair, whom you gonna call?']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'production',\n",
       " 'fails',\n",
       " 'in',\n",
       " 'dipsair,',\n",
       " 'whom',\n",
       " 'you',\n",
       " 'gonna',\n",
       " 'call?',\n",
       " 'Just',\n",
       " 'an',\n",
       " 'info',\n",
       " 'message',\n",
       " 'passing',\n",
       " 'by',\n",
       " 'Just',\n",
       " 'an',\n",
       " 'info',\n",
       " 'message',\n",
       " 'passing']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.flatMap(lambda x: x[2].split()).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = logs.flatMap(lambda x: x[2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'production',\n",
       " 'fails',\n",
       " 'in',\n",
       " 'dipsair,',\n",
       " 'whom',\n",
       " 'you',\n",
       " 'gonna',\n",
       " 'call?',\n",
       " 'Just']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны отдельные трансформации и отдельные action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/dag1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/dag2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Последовательность трансформаций определяет граф вычислений (DAG - direct acyclic graph). В нем есть партиции и зависимости между партициями. Таким образом Spark имеет всю необходимую информацию для вычилсения графа в любой точке и возможных оптимизаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/dag3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформации бывают *узкими*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/narrow_transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И *широкими*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/wide_transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Широкие трансформации разделяют джоб на стейджи. Между стейджами происходит shuffle данных, которого надо избегать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Персистентность и кэширование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD вычисляются лениво, когда вызывается action. Часто мы хотим вызвать несколько actions для одного и тоге же RDD. Если мы просто сделаем это, то граф будет полностью перевычисляться каждый раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.3.99', 'USA'],\n",
       " ['192.168.3.98', 'USA'],\n",
       " ['192.168.3.97', 'USA'],\n",
       " ['192.168.3.96', 'USA'],\n",
       " ['192.168.3.95', 'USA'],\n",
       " ['192.168.3.94', 'USA'],\n",
       " ['192.168.3.93', 'USA'],\n",
       " ['192.168.3.92', 'USA'],\n",
       " ['192.168.3.91', 'USA'],\n",
       " ['192.168.3.90', 'USA']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтобы этого избежать, мы можем закэшировать RDD в памяти. Кэширование произойдет при вызове первого action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_cached = ips.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_cached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.3.99', 'USA'],\n",
       " ['192.168.3.98', 'USA'],\n",
       " ['192.168.3.97', 'USA'],\n",
       " ['192.168.3.96', 'USA'],\n",
       " ['192.168.3.95', 'USA'],\n",
       " ['192.168.3.94', 'USA'],\n",
       " ['192.168.3.93', 'USA'],\n",
       " ['192.168.3.92', 'USA'],\n",
       " ['192.168.3.91', 'USA'],\n",
       " ['192.168.3.90', 'USA'],\n",
       " ['192.168.3.9', 'USA'],\n",
       " ['192.168.3.89', 'USA'],\n",
       " ['192.168.3.88', 'USA'],\n",
       " ['192.168.3.87', 'USA'],\n",
       " ['192.168.3.86', 'USA'],\n",
       " ['192.168.3.85', 'USA'],\n",
       " ['192.168.3.84', 'USA'],\n",
       " ['192.168.3.83', 'USA'],\n",
       " ['192.168.3.82', 'USA'],\n",
       " ['192.168.3.81', 'USA']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_cached.top(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cache()` сохраняет RDD в памяти. Для большего контроля можно использовать `persist(storage_level)`:\n",
    "+ MEMORY_ONLY\n",
    "+ MEMORY_AND_DISK\n",
    "+ DISK_ONLY\n",
    "+ MEMORY_ONLY_2\n",
    "+ MEMORY_AND_DISK_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Все сохраненные RDD можно увидеть во вкладке \"Storage\" Spark UI\n",
    "### Или более программатичным способом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StorageLevel(False, True, False, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.persist(StorageLevel.DISK_ONLY_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, False, False, False, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairRDD (ключ-значение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PairRDD - это RDD для работы с парами ключ-значение. Spark предполагает, что PairRDD содержить в себе объекты, состящие ровно из двух элементов! PairRDD предоставляют методы группировки, аггрегации и объединения (join) двух RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пусть есть задача подсчитать распределение кодов ERROR и WARNING в лог-файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['192.168.0.10\\tERROR\\tWhen production fails in dipsair, whom you gonna call?',\n",
       " '192.168.0.39\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.35\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.19\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.23\\tERROR\\tWhen production fails in dipsair, whom you gonna call?']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_logs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WARNING', <pyspark.resultiterable.ResultIterable at 0x7fa5203dd350>),\n",
       " ('ERROR', <pyspark.resultiterable.ResultIterable at 0x7fa5203dd410>)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_logs.filter(lambda x: \"INFO\" not in x)\n",
    "         .map(lambda x: (x.split(\"\\t\")[1], 1))\\\n",
    "         .groupByKey()\n",
    "         .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WARNING', 250004), ('ERROR', 49948)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_logs.filter(lambda x: \"INFO\" not in x)\n",
    "         .map(lambda x: (x.split(\"\\t\")[1], 1))\\\n",
    "         .groupByKey()\n",
    "         .map(lambda x: (x[0], len(x[1])))\n",
    "         .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Или немного проще"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('ERROR', 49948), ('WARNING', 250004)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_logs.filter(lambda x: \"INFO\" not in x)\n",
    "         .map(lambda x: (x.split(\"\\t\")[1], 1))\n",
    "         .countByKey()\n",
    "         .items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоит заметить, что `groupByKey()` предполагает перемещение всех записей с одним ключом на один экзекьютор. В случае очень скоршенных распределений это может привести к падению экзекьютора с OOM. Поэтому всегда при группировках стоит подумать об использовании `reduceByKey()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_logs.filter(lambda x: \"INFO\" not in x)\n",
    "         .map(lambda x: (x.split(\"\\t\")[1], 1))\\\n",
    "         .reduceByKey(lambda x, y: x + y)\n",
    "         .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Два PairRDD можно объединить по ключу\n",
    "### Поддерживаются inner join, left outer join, right outer join и full outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.10',\n",
       "  'ERROR',\n",
       "  'When production fails in dipsair, whom you gonna call?'],\n",
       " ['192.168.0.39', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.35', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.19', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.23',\n",
       "  'ERROR',\n",
       "  'When production fails in dipsair, whom you gonna call?']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.1', 'CHINA'],\n",
       " ['192.168.0.2', 'CHINA'],\n",
       " ['192.168.0.3', 'CHINA'],\n",
       " ['192.168.0.4', 'CHINA'],\n",
       " ['192.168.0.5', 'CHINA']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.23', 'CHINA']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.filter(lambda x: x[0] == '192.168.0.23').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('192.168.0.23', ('ERROR', 'CHINA')),\n",
       " ('192.168.0.23', ('INFO', 'CHINA')),\n",
       " ('192.168.0.23', ('INFO', 'CHINA')),\n",
       " ('192.168.0.23', ('ERROR', 'CHINA')),\n",
       " ('192.168.0.23', ('INFO', 'CHINA'))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.join(ips).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/Jackie-Chan-WTF.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не стоит забывать, что Spark предполагает, что PairRDD состоит ровно! из двух элементов, поэтому все остальные элементы просто отбрасываются!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_logs(line):\n",
    "    split = line.split(\"\\t\")\n",
    "    return split[0], split[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_cached = raw_logs.map(split_logs).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('192.168.0.10',\n",
       "  ['ERROR', 'When production fails in dipsair, whom you gonna call?']),\n",
       " ('192.168.0.39', ['INFO', 'Just an info message passing by']),\n",
       " ('192.168.0.35', ['INFO', 'Just an info message passing by']),\n",
       " ('192.168.0.19', ['INFO', 'Just an info message passing by']),\n",
       " ('192.168.0.23',\n",
       "  ['ERROR', 'When production fails in dipsair, whom you gonna call?'])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_cached.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('192.168.0.4', (['INFO', 'Just an info message passing by'], 'CHINA')),\n",
       " ('192.168.0.4', (['WARNING', 'Something bad could happen'], 'CHINA')),\n",
       " ('192.168.0.4', (['WARNING', 'Something bad could happen'], 'CHINA')),\n",
       " ('192.168.0.4', (['INFO', 'Just an info message passing by'], 'CHINA')),\n",
       " ('192.168.0.4', (['WARNING', 'Something bad could happen'], 'CHINA'))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_cached.join(ips).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Управление параллелизмом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомним, что атомарным уровнем параллелизма в Spark является партиция. Об этом всегда стоит помнить, когда есть проблемы с производительностью приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод `repartition()` может быть использован для изменения числа партиций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `repartition()` всегда приводит к равномерному перераспределению данных, что ведет к shuffle. Если Вы уменьшаете число партиций, то стоит использовать `coalesce()`, который может избежать shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.coalesce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.coalesce(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Узнать дефолтный уровень параллелизма можно из конфига. По-умолчанию, при работе с YARN, использукется общее число ядер, выделенных этому SparkContext на всех экзекьюторах, либо 2. Что больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf().get(\"spark.default.parallelism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(range(100)).getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast-объект - это неизменяемый объект, которая разделяется между всеми экзекьюторами\n",
    "### Дистрибуция broadcast-объекта производится быстро и эффективно p2p-протоколом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуем map-side join с помощью broadcast-объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_local = dict(ips.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHINA'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_local['192.168.0.10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_broadcasted = sc.broadcast(ips_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7fa503c53c10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_broadcasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHINA'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_broadcasted.value['192.168.0.10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_cached.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_ip(row):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHINA',\n",
       "  (['ERROR', 'When production fails in dipsair, whom you gonna call?'],)),\n",
       " ('CHINA', (['INFO', 'Just an info message passing by'],)),\n",
       " ('CHINA', (['INFO', 'Just an info message passing by'],)),\n",
       " ('CHINA', (['INFO', 'Just an info message passing by'],)),\n",
       " ('CHINA',\n",
       "  (['ERROR', 'When production fails in dipsair, whom you gonna call?'],)),\n",
       " ('CHINA',\n",
       "  (['ERROR', 'When production fails in dipsair, whom you gonna call?'],)),\n",
       " ('CHINA', (['INFO', 'Just an info message passing by'],)),\n",
       " ('CHINA', (['WARNING', 'Something bad could happen'],)),\n",
       " ('CHINA', (['INFO', 'Just an info message passing by'],)),\n",
       " ('CHINA', (['INFO', 'Just an info message passing by'],))]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_cached.map(resolve_ip).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Не забудьте погасить SparkContext!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
