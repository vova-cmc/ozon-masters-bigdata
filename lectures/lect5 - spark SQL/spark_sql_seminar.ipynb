{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подсчет числа пользовательских сессий\n",
    "Вам необходимо подсчитать число пользовательских сессий в разбивке по доменам на данных из лог-файла.\n",
    "\n",
    "**Пользовательская сессия** - это пребывание пользователя на сайте такое, что между двумя последовательными кликами проходит не более 30 минут.\n",
    "Лог-файл такой же, как и на лекции. Находится в HDFS по пути `/datasets/spark/logsM.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI port: 10232\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME = \"/usr/hdp/current/spark2-client\"\n",
    "PYSPARK_PYTHON = \"/opt/conda/envs/dsenv/bin/python\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]= PYSPARK_PYTHON\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "PYSPARK_HOME = os.path.join(SPARK_HOME, \"python/lib\")\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"py4j-0.10.7-src.zip\"))\n",
    "sys.path.insert(0, os.path.join(PYSPARK_HOME, \"pyspark.zip\"))\n",
    "\n",
    "import random\n",
    "SPARK_UI_PORT = random.choice(range(10000, 11000))\n",
    "print(f\"Spark UI port: {SPARK_UI_PORT}\")\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.ui.port\", SPARK_UI_PORT)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Spark SQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dsenv/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №1\n",
    "Создайте `DataFrame` из лог-файла. Схему можно скопировать из лекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "log_schema = StructType(fields=[\n",
    "    StructField(\"ip\", StringType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"size\", IntegerType()),\n",
    "    StructField(\"code\", IntegerType()),\n",
    "    StructField(\"ua\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ip: string, timestamp: bigint, url: string, size: int, code: int, ua: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = spark.read.csv('/datasets/spark/logsM.txt', sep=\"\\t\", schema=log_schema)\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+----+----+--------------------+\n",
      "|             ip|     timestamp|                 url|size|code|                  ua|\n",
      "+---------------+--------------+--------------------+----+----+--------------------+\n",
      "|  33.49.147.163|20140101014611|http://news.rambl...| 378| 431|Safari/5.0 (compa...|\n",
      "| 197.72.248.141|20140101020306|http://news.mail....|1412| 203|Safari/5.0 (compa...|\n",
      "|  33.49.147.163|20140101023103|http://lenta.ru/4...|1189| 451|Chrome/5.0 (compa...|\n",
      "|  75.208.40.166|20140101032909|http://newsru.com...|  60| 306|Safari/5.0 (Windo...|\n",
      "| 197.72.248.141|20140101033626|http://newsru.com...| 736| 307|Chrome/5.0 (compa...|\n",
      "| 222.131.187.37|20140101033837|http://news.mail....|1017| 416|Opera/5.0 (compat...|\n",
      "| 197.72.248.141|20140101034726|http://news.rambl...|2042| 428|Safari/5.0 (compa...|\n",
      "|  33.49.147.163|20140101041149|http://lenta.ru/5...| 444| 203|Chrome/5.0 (compa...|\n",
      "| 197.72.248.141|20140101050543|http://news.yande...|1197| 500|Chrome/5.0 (Windo...|\n",
      "| 181.217.177.35|20140101052930|http://lenta.ru/4...|1976| 100|Safari/5.0 compat...|\n",
      "|247.182.249.253|20140101054133|http://news.rambl...| 796| 426|Firefox/5.0 (comp...|\n",
      "| 181.217.177.35|20140101060418|http://newsru.com...|1091| 504|Chrome/5.0 (compa...|\n",
      "| 56.167.169.126|20140101064553|http://newsru.com...|1985| 449|Opera/5.0 (compat...|\n",
      "|  33.49.147.163|20140101065321|http://news.rambl...|1155| 422|Safari/5.0 (compa...|\n",
      "|   49.203.96.67|20140101065554|http://news.yande...| 398| 503|Opera/5.0 (compat...|\n",
      "|  75.208.40.166|20140101071725|http://news.rambl...|  20| 303|Opera/5.0 (Window...|\n",
      "| 56.167.169.126|20140101073452|http://news.rambl...|1286| 414|Opera/5.0 (compat...|\n",
      "|  75.208.40.166|20140101073934|http://lenta.ru/5...|1180| 415|Safari/5.0 (compa...|\n",
      "|  75.208.40.166|20140101074200|http://lenta.ru/8...| 808| 306|Chrome/5.0 compat...|\n",
      "|  75.208.40.166|20140101080122|http://news.mail....| 877| 301|Chrome/5.0 (compa...|\n",
      "+---------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №2\n",
    "Лог не содержит столбца с доменом. Конечно можно извлечь домен с помощью функции [regexp_extract](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_extract), но мы так делать не будем. Напишите `pandas_udf`, которая будет извлекать домены из столбца `url`. Результаты применения функции поместите в столбец `domain`.\n",
    "\n",
    "Для извлечения домена можно воспользоваться функцией [urlparse](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "import pyspark.sql.functions as f\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def _get_domain(url):\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "@f.pandas_udf(StringType())\n",
    "def get_domain(urls):\n",
    "    return (urls.apply(_get_domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = log.withColumn('domain', get_domain('url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------+----+----+--------------------+---------------+\n",
      "|            ip|     timestamp|                 url|size|code|                  ua|         domain|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+---------------+\n",
      "| 33.49.147.163|20140101014611|http://news.rambl...| 378| 431|Safari/5.0 (compa...|news.rambler.ru|\n",
      "|197.72.248.141|20140101020306|http://news.mail....|1412| 203|Safari/5.0 (compa...|   news.mail.ru|\n",
      "| 33.49.147.163|20140101023103|http://lenta.ru/4...|1189| 451|Chrome/5.0 (compa...|       lenta.ru|\n",
      "| 75.208.40.166|20140101032909|http://newsru.com...|  60| 306|Safari/5.0 (Windo...|     newsru.com|\n",
      "|197.72.248.141|20140101033626|http://newsru.com...| 736| 307|Chrome/5.0 (compa...|     newsru.com|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №3\n",
    "Для разминки давайте подсчитаем сколько дней прошло между первым и последним посещением пользователем нашего домена. Будем считать, что интересующий нас домен `news.mail.ru`. В качестве \"уникального\" идентификатора пользователя договоримся использовать ip-адрес. Использовать оконные функции в данном задании не надо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №3.1\n",
    "Для выполнения задания №3 понадобится делать операции с датами. Заметьте, что в столбце `timestamp` хранится не настоящий timestamp, а число с датой в формате \"yyyyMMddHHmmss\". Используя функции из `pyspark.sql.functions`, создайте новый столбец `timestamp`, содержащий в себе UNIX timestamp.\n",
    "\n",
    "При возникновении ошибок, обратите внимание на типы данных. Возможно их нужно привести."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "# def unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss'):\n",
    "\n",
    "\n",
    "def _to_unix_timestamp(ts):\n",
    "    return f.unix_timestamp(str(ts), format='yyyyMMddHHmmss')\n",
    "\n",
    "@f.pandas_udf(StringType())\n",
    "def to_unix_timestamp(urls):\n",
    "    return (urls.apply(_get_domain))\n",
    "f.unix_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = log.filter(log.domain == 'news.mail.ru')\n",
    "log = log.withColumn('timestamp', f.unix_timestamp(f.col('timestamp').cast('string'), 'yyyyMMddHHmmss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+----+----+--------------------+------------+\n",
      "|            ip| timestamp|                 url|size|code|                  ua|      domain|\n",
      "+--------------+----------+--------------------+----+----+--------------------+------------+\n",
      "|197.72.248.141|1388541786|http://news.mail....|1412| 203|Safari/5.0 (compa...|news.mail.ru|\n",
      "|222.131.187.37|1388547517|http://news.mail....|1017| 416|Opera/5.0 (compat...|news.mail.ru|\n",
      "| 75.208.40.166|1388563282|http://news.mail....| 877| 301|Chrome/5.0 (compa...|news.mail.ru|\n",
      "| 33.49.147.163|1388568276|http://news.mail....| 732| 409|Opera/5.0 (compat...|news.mail.ru|\n",
      "|110.91.102.196|1388569315|http://news.mail....| 448| 201|Chrome/5.0 (compa...|news.mail.ru|\n",
      "+--------------+----------+--------------------+----+----+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №3.2\n",
    "Приведя timestamp к правильному формату, решите исходную задачу. В результате должен получится `DataFrame` с двумя столбцами `ip` и `days`. Отсортируйте результат по столбцу `days` в порядке убывания и выведите первые 20 строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №4\n",
    "Подсчитайте число сессий, которое каждый пользователь (уникальный ip) сделал на домене `news.mail.ru`. Для решения этой задачи потребуется использование оконных функций (что это такое чуть ниже). Для работы с окнами в Spark SQL используется метод [over()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.over). Само окно определяется с помощью класса [Window](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window). Резудьтатом будет `DataFrame` со столбцами `ip` и `sessions`, отсортированный в порядке убывания числа сессий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Оконная функция_ выполняет вычисления для набора строк, некоторым образом связанных с текущей строкой. Можно сравнить её с агрегатной функцией, но, в отличие от обычной агрегатной функции, при использовании оконной функции несколько строк не группируются в одну, а продолжают существовать отдельно. Внутри же, оконная функция, как и агрегатная, может обращаться не только к текущей строке результата запроса.\n",
    "\n",
    "![](https://www.sqlitetutorial.net/wp-content/uploads/2018/11/SQLite-window-function-vs-aggregate-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот пример, показывающий, как сравнить зарплату каждого сотрудника со средней зарплатой его отдела:\n",
    "\n",
    "```sql\n",
    "SELECT depname, empno, salary, avg(salary) OVER (PARTITION BY depname)\n",
    "  FROM empsalary;\n",
    "```\n",
    "\n",
    "```\n",
    "  depname  | empno | salary |          avg          \n",
    "-----------+-------+--------+-----------------------\n",
    " develop   |    11 |   5200 | 5020.0000000000000000\n",
    " develop   |     7 |   4200 | 5020.0000000000000000\n",
    " develop   |     9 |   4500 | 5020.0000000000000000\n",
    " develop   |     8 |   6000 | 5020.0000000000000000\n",
    " develop   |    10 |   5200 | 5020.0000000000000000\n",
    " personnel |     5 |   3500 | 3700.0000000000000000\n",
    " personnel |     2 |   3900 | 3700.0000000000000000\n",
    " sales     |     3 |   4800 | 4866.6666666666666667\n",
    " sales     |     1 |   5000 | 4866.6666666666666667\n",
    " sales     |     4 |   4800 | 4866.6666666666666667\n",
    "(10 rows)\n",
    "```\n",
    "\n",
    "[Документация PostgreSQL](https://postgrespro.ru/docs/postgrespro/12/tutorial-window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_window = Window.partitionBy(\"ip\").orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|             ip|sessions|\n",
      "+---------------+--------+\n",
      "|  75.208.40.166|     349|\n",
      "| 197.72.248.141|     283|\n",
      "|  33.49.147.163|     246|\n",
      "| 222.131.187.37|     163|\n",
      "|135.124.143.193|     149|\n",
      "| 168.255.93.197|     139|\n",
      "| 56.167.169.126|     129|\n",
      "|   49.203.96.67|     117|\n",
      "|   49.105.15.79|     113|\n",
      "| 110.91.102.196|      74|\n",
      "+---------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(\"ip\", \"timestamp\", f.lead(\"timestamp\").over(user_window).alias(\"lead\"))\\\n",
    "    .select(\"ip\", \"timestamp\", (f.col(\"timestamp\") - f.col(\"lead\")).alias(\"diff\"))\\\n",
    "    .where(\"diff <= -1800 or diff is NULL\")\\\n",
    "    .groupBy(\"ip\").agg(f.count(\"*\").alias(\"sessions\"))\\\n",
    "    .orderBy(f.col(\"sessions\").desc())\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №5\n",
    "Нарисуйте гистограмму распределения числа сессий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
